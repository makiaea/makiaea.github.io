<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
<title>ac7fab18a3cafbbcd12992dc28a734d9</title>
<script type="text/javascript">
function showhide(id)
{
    var e = document.getElementById(id);
    e.style.display = (e.style.display == 'block') ? 'none' : 'block';
}
</script>
<style>
rect {
transition: .6s fill;
fill: #D3D3D3;
opacity: 0;
}
rect:hover {
    fill: #D3D3D3;
    opacity: 0.2;
}

.outline {
    clear: both;
}

.svg-container {
    width: 100%;
    max-width: 2920px;
    margin-left: auto;
    margin-right: auto;
    display: block;
}

.svg-content {
    width: 100%;
}

.container {
    width: 100%;
}
</style>

</head>

<body>

<div class="container"><div class="spacer">&nbsp;</div><div class="svg-container"><svg version="1.1"  preserveAspectRatio="xMinYMin meet" class="svg-content" viewBox="0 0 2920 8192" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<image width="2920" height="8192" xlink:href="00002-ac7fab18a3cafbbcd12992dc28a734d9-assets/00002-ac7fab18a3cafbbcd12992dc28a734d9.png"></image>
<a xlink:href="http://doi.org/10.1371/journal.pone.0188764" xlink:title="http://doi.org/10.1371/journal.pone.0188764"><rect x="1818" y="486" width="364" height="26"/></a>
<a xlink:href="http://doi.org/10.1038/s41586-019-1015-8" xlink:title="http://doi.org/10.1038/s41586-019-1015-8"><rect x="1818" y="990" width="353" height="26"/></a>
<a xlink:href="https://www.amazon.co.uk/gp/aw/d/B00BBCTQK6/ref=pd_aw_sims_1?pi=SL500_SY115" xlink:title="https://www.amazon.co.uk/gp/aw/d/B00BBCTQK6/ref=pd_aw_sims_1?pi=SL500_SY115"><rect x="2182" y="1161" width="72" height="26"/></a>
<a xlink:href="https://www.amazon.co.uk/gp/aw/d/B00BBCTQHY/ref=mp_s_a_1_17?qid=1464332438&sr=8-17&pi=AC_SX236_SY340_QL65&keywords=ear+defenders" xlink:title="https://www.amazon.co.uk/gp/aw/d/B00BBCTQHY/ref=mp_s_a_1_17?qid=1464332438&amp;sr=8-17&amp;pi=AC_SX236_SY340_QL65&amp;keywords=ear+defenders"><rect x="2182" y="1268" width="72" height="26"/></a>
<a xlink:href="https://doi.org/10.1177%2F0305735612463948" xlink:title="https://doi.org/10.1177%2F0305735612463948"><rect x="1818" y="1595" width="389" height="26"/></a>
<a xlink:href="https://dl3.ankiweb.net/shared/downloadDeck2/789434294?k=Wzc4OTQzNDI5NCwgMjM3NzMsIDczNjI3N10.lPQ6XP3n5iB_9kt-XoHUCVmaXnzz2aWrCixSmrTkUUk" xlink:title="https://dl3.ankiweb.net/shared/downloadDeck2/789434294?k=Wzc4OTQzNDI5NCwgMjM3NzMsIDczNjI3N10.lPQ6XP3n5iB_9kt-XoHUCVmaXnzz2aWrCixSmrTkUUk"><rect x="1818" y="1894" width="76" height="26"/></a>
<a xlink:href="http://youtube.com/watch?v=vc9qh709gas" xlink:title="http://youtube.com/watch?v=vc9qh709gas"><rect x="1818" y="2031" width="74" height="26"/></a>
<a xlink:href="http://doi.org/10.1103/physrevb.99.024302" xlink:title="http://doi.org/10.1103/physrevb.99.024302"><rect x="1818" y="2254" width="353" height="26"/></a>
<a xlink:href="https://aeon.co/ideas/old-school-news-sensational-moralistic-and-above-all-sung" xlink:title="https://aeon.co/ideas/old-school-news-sensational-moralistic-and-above-all-sung"><rect x="1818" y="2429" width="449" height="26"/></a>
<a xlink:href="https://aeon.co/ideas/old-school-news-sensational-moralistic-and-above-all-sung" xlink:title="https://aeon.co/ideas/old-school-news-sensational-moralistic-and-above-all-sung"><rect x="1818" y="2455" width="272" height="26"/></a>
<a xlink:href="http://doi.org/10.7554/elife.47027" xlink:title="http://doi.org/10.7554/elife.47027"><rect x="1818" y="2658" width="262" height="26"/></a>
<a xlink:href="http://doi.org/10.1021/acsomega.8b01673" xlink:title="http://doi.org/10.1021/acsomega.8b01673"><rect x="1818" y="3568" width="348" height="26"/></a>
<a xlink:href="http://doi.org/10.1021/acsnano.9b02180" xlink:title="http://doi.org/10.1021/acsnano.9b02180"><rect x="1818" y="3749" width="330" height="26"/></a>
<a xlink:href="http://doi.org/10.1038/s41598-018-27913-0" xlink:title="http://doi.org/10.1038/s41598-018-27913-0"><rect x="1818" y="4036" width="365" height="26"/></a>
<a xlink:href="http://doi.org/10.1371/journal.pone.0201766" xlink:title="http://doi.org/10.1371/journal.pone.0201766"><rect x="1818" y="4324" width="364" height="26"/></a>
<a xlink:href="http://doi.org/10.1029/2018GL079665" xlink:title="http://doi.org/10.1029/2018GL079665"><rect x="1818" y="4480" width="309" height="26"/></a>
<a xlink:href="https://videolan.org/vlc/" xlink:title="https://videolan.org/vlc/"><rect x="2143" y="4824" width="151" height="26"/></a>
<a xlink:href="https://apps.apple.com/gb/app/anchor/id1056182234" xlink:title="https://apps.apple.com/gb/app/anchor/id1056182234"><rect x="1818" y="5041" width="443" height="26"/></a>
<a xlink:href="http://www.musescore.com" xlink:title="http://www.musescore.com"><rect x="1818" y="5531" width="150" height="26"/></a>
<a xlink:href="https://apps.apple.com/gb/app/necrodancer-amplified/id1445623416" xlink:title="https://apps.apple.com/gb/app/necrodancer-amplified/id1445623416"><rect x="2005" y="5662" width="446" height="26"/></a>
<a xlink:href="https://apps.apple.com/gb/app/necrodancer-amplified/id1445623416" xlink:title="https://apps.apple.com/gb/app/necrodancer-amplified/id1445623416"><rect x="2005" y="5689" width="139" height="26"/></a>
<a xlink:href="https://audiokitpro.com" xlink:title="https://audiokitpro.com"><rect x="1818" y="5878" width="147" height="26"/></a>
<a xlink:href="https://itunes.apple.com/gb/app/audiokit-synth-one-synthesizer/id1371050497?mt=8" xlink:title="https://itunes.apple.com/gb/app/audiokit-synth-one-synthesizer/id1371050497?mt=8"><rect x="2067" y="5813" width="421" height="26"/></a>
<a xlink:href="https://itunes.apple.com/gb/app/audiokit-synth-one-synthesizer/id1371050497?mt=8" xlink:title="https://itunes.apple.com/gb/app/audiokit-synth-one-synthesizer/id1371050497?mt=8"><rect x="2067" y="5839" width="248" height="26"/></a>
<a xlink:href="https://itunes.apple.com/gb/app/fm-player-classic-dx-synths/id1307785646?mt=8" xlink:title="https://itunes.apple.com/gb/app/fm-player-classic-dx-synths/id1307785646?mt=8"><rect x="2067" y="5914" width="437" height="26"/></a>
<a xlink:href="https://itunes.apple.com/gb/app/fm-player-classic-dx-synths/id1307785646?mt=8" xlink:title="https://itunes.apple.com/gb/app/fm-player-classic-dx-synths/id1307785646?mt=8"><rect x="2067" y="5941" width="206" height="26"/></a>
<a xlink:href="https://kid3.sourceforge.io" xlink:title="https://kid3.sourceforge.io"><rect x="2116" y="5991" width="176" height="26"/></a>
<a xlink:href="http://www.macworld.com/article/2079194/how-to-create-ios-ringtones.html" xlink:title="http://www.macworld.com/article/2079194/how-to-create-ios-ringtones.html"><rect x="2008" y="6120" width="22" height="22"/></a>
<a xlink:href="https://www.youtube.com/watch?v=lzq2SrDnP1E" xlink:title="https://www.youtube.com/watch?v=lzq2SrDnP1E"><rect x="2033" y="6183" width="22" height="22"/></a>
<a xlink:href="https://help.apple.com/logicremote/ipad/1.0/#" xlink:title="https://help.apple.com/logicremote/ipad/1.0/#"><rect x="2041" y="6240" width="362" height="26"/></a>
<a xlink:href="http://web.audacityteam.org/" xlink:title="http://web.audacityteam.org/"><rect x="1818" y="6313" width="211" height="26"/></a>
<a xlink:href="https://rogueamoeba.com/airfoil/mac/" xlink:title="https://rogueamoeba.com/airfoil/mac/"><rect x="1818" y="6433" width="284" height="26"/></a>
<a xlink:href="https://itunes.apple.com/gb/app/just-press-record/id1033342465?mt=8" xlink:title="https://itunes.apple.com/gb/app/just-press-record/id1033342465?mt=8"><rect x="1818" y="6842" width="408" height="26"/></a>
<a xlink:href="https://itunes.apple.com/gb/app/just-press-record/id1033342465?mt=8" xlink:title="https://itunes.apple.com/gb/app/just-press-record/id1033342465?mt=8"><rect x="1818" y="6869" width="199" height="26"/></a>
<a xlink:href="https://auphonic.com/pricing" xlink:title="https://auphonic.com/pricing"><rect x="1818" y="6970" width="200" height="26"/></a>
<a xlink:href="http://www.wooji-juice.com/products/ferrite/" xlink:title="http://www.wooji-juice.com/products/ferrite/"><rect x="1818" y="7068" width="297" height="26"/></a>
<a xlink:href="https://www.youtube.com/user/woojijuice" xlink:title="https://www.youtube.com/user/woojijuice"><rect x="2303" y="7045" width="265" height="26"/></a>
<a xlink:href="https://youtu.be/0K4M7dy1Ehg" xlink:title="https://youtu.be/0K4M7dy1Ehg"><rect x="1818" y="7165" width="226" height="26"/></a>
<a xlink:href="https://imazing.com/" xlink:title="https://imazing.com/"><rect x="2219" y="7238" width="123" height="26"/></a>
<a xlink:href="https://imazing.com/store/educational" xlink:title="https://imazing.com/store/educational"><rect x="2219" y="7311" width="282" height="26"/></a>
<a xlink:href="https://busk.co/blog/" xlink:title="https://busk.co/blog/"><rect x="1818" y="8133" width="129" height="26"/></a>
<a xlink:href="01002-4674e950f17b2199110ac59976c9bb7a.html" xlink:title="01002-4674e950f17b2199110ac59976c9bb7a.html"><rect x="1441" y="3139" width="22" height="22"/></a>
<a xlink:href="http://doi.org/10.1126/science.aav3218" xlink:title="http://doi.org/10.1126/science.aav3218"><rect x="864" y="2868" width="318" height="26"/></a>
</svg>
</div>
</div>

<br/>
<a href="javascript:showhide('outlineDiv')">Outline show/hide</a>
<div id="outlineDiv" style="display:none;">
<h1>00002</h1>
<p>priss</p>
<h2>05002</h2>
<ul>
<li>
<p><em>cell type-specific suppression of mechanosensitive genes by audible sound stimulation</em><br />
masahiro kumeta et al. 2018<br />
<a href="http://doi.org/10.1371/journal.pone.0188764">doi.org/10.1371/journal.pone.0188764</a></p>
<ul>
<li>Audible sound is a ubiquitous environmental factor in nature that transmits oscillatory compressional pressure through the substances. To investigate the property of the sound as a mechanical stimulus for cells, an experimental system was set up using 94.0 dB sound which transmits approximately 10 mPa pressure to the cultured cells. Based on research on mechanotransduction and ultrasound effects on cells, gene responses to the audible sound stimulation were analyzed by varying several sound parameters: frequency, wave form, composition, and exposure time. Real-time quantitative PCR analyses revealed a distinct suppressive effect for several mechanosensitive and ultrasound-sensitive genes that were triggered by sounds. The effect was clearly observed in a wave form- and pressure level-specific manner, rather than the frequency, and persisted for several hours. At least two mechanisms are likely to be involved in this sound response: transcriptional control and RNA degradation. ST2 stromal cells and C2C12 myoblasts exhibited a robust response, whereas NIH3T3 cells were partially and NB2a neuroblastoma cells were completely insensitive, suggesting a cell type-specific response to sound. These findings reveal a cell-level systematic response to audible sound and uncover novel relationships between life and sound.</li>
</ul>
</li>
<li>
<p><em>feature-selective encoding of substrate vibrations in the forelimb somatosensory cortex</em><br />
mario prsa et al. 2019<br />
<a href="http://doi.org/10.1038/s41586-019-1015-8">doi.org/10.1038/s41586-019-1015-8</a></p>
<ul>
<li>
<p>Using two-photon microscopy, Daniel Huber's team visualized the activity of hundreds of neurons in a mouse's somatosensory cortex as vibrations of different frequencies were delivered to its forepaw. Like in the auditory cortex, individual neurons were selectively tuned: they strongly responded to some frequencies and less so to others. &quot;It turns out that these neurons are preferentially tuned to a specific combination of frequency and amplitude, and that this combination corresponds to what the mouse actually perceives. In other words, a mouse is unable to distinguish a high-frequency vibration with a low amplitude from a low-frequency vibration with a higher amplitude,&quot; explains Mario Prsa, a researcher in Dr. Huber's team and the study's first author. &quot;It is the same psychoacoustic effect detected in the auditory system, where the perceived pitch of a sound changes with both frequency and loudness.&quot; Thus, despite the fact that sounds -- which travel through the air -- and vibrations -- which are transmitted through solid matter -- are processed by different sensory channels, they are both perceived and encoded similarly in the brain.</p>
<p>Everything goes through Pacinian corpuscles</p>
<p>In a second step, the researchers sought to identify the origin of the somatosensory stimuli involved by performing a detailed histological analysis of Pacinian corpuscles in mouse forelimb. Pacinian corpuscles are known to transduce high frequency vibrations in mammals and are densely expressed in the dermis of primate fingertips. &quot;Surprisingly, we found that the vibration responses in the mouse brain stem from Pacinian corpuscles located on the forearm bones, whereas they were totally absent in the paw's skin,&quot; explains Géraldine Cuenu, a student in the UNIGE master's programme in neurosciences, who took charge of this detailed analysis. Using optogenetics, the scientists confirmed the link between cortical responses and the particular configuration of mechanoreceptors in the forelimbs.</p>
<p>An ancestor of the hearing system?</p>
<p>Could it be that the particular distribution of vibration-sensitive mechanoreceptors along the bones of the forelimb acts as a seismograph to &quot;listen&quot; to vibrations? Vibratory stimuli are indeed used by a number of living organisms to communicate through plants, branches and other solid substrates. &quot;Our discoveries probably reveal the existence of an ancient sensory channel, which could be an evolutionary precursor of hearing,&quot; concludes Mario Prsa. This somewhat vestigial, yet highly sensitive modality might also explain how we are able to identify subtle clues linked to upcoming natural disasters, or why construction or traffic causes nuisances even when inaudible.</p>
</li>
<li>
<p><em>abstract</em> The spectral content of skin vibrations, produced by either displacing the finger across a surface texture<a href="https://www.nature.com/articles/s41586-019-1015-8#ref-CR1">1</a> or passively sensing external movements through the solid substrate<a href="https://www.nature.com/articles/s41586-019-1015-8#ref-CR2">2</a>,<a href="https://www.nature.com/articles/s41586-019-1015-8#ref-CR3">3</a>, provides fundamental information about our environment. Low-frequency flutter (below 50 Hz) applied locally to the primate fingertip evokes cyclically entrained spiking in neurons of the primary somatosensory cortex (S1), and thus spike rates in these neurons increase linearly with frequency<a href="https://www.nature.com/articles/s41586-019-1015-8#ref-CR4">4</a>,<a href="https://www.nature.com/articles/s41586-019-1015-8#ref-CR5">5</a>. However, the same local vibrations at high frequencies (over 100 Hz) cannot be discriminated on the basis of differences in discharge rates of S1 neurons<a href="https://www.nature.com/articles/s41586-019-1015-8#ref-CR4">4</a>,<a href="https://www.nature.com/articles/s41586-019-1015-8#ref-CR6">6</a>, because spiking is only partially entrained at these frequencies<a href="https://www.nature.com/articles/s41586-019-1015-8#ref-CR6">6</a>. Here we investigated whether high-frequency substrate vibrations applied broadly to the mouse forelimb rely on a different cortical coding scheme. We found that forelimb S1 neurons encode vibration frequency similarly to sound pitch representation in the auditory cortex<a href="https://www.nature.com/articles/s41586-019-1015-8#ref-CR7">7</a>,<a href="https://www.nature.com/articles/s41586-019-1015-8#ref-CR8">8</a>: their spike rates are selectively tuned to a preferred value of a low-level stimulus feature without any temporal entrainment. This feature, identified as the product of frequency and a power function of amplitude, was also found to be perceptually relevant as it predicted behaviour in a frequency discrimination task. Using histology, peripheral deafferentation and optogenetic receptor tagging, we show that these selective responses are inherited from deep Pacinian corpuscles located adjacent to bones, most densely around the ulna and radius and only sparsely along phalanges. This mechanoreceptor arrangement and the tuned cortical rate code suggest that the mouse forelimb constitutes a sensory channel best adapted for passive ‘listening’ to substrate vibrations, rather than for active texture exploration.</p>
</li>
</ul>
</li>
</ul>
<h2>05102</h2>
<ul>
<li>when singing with earphones
<ul>
<li>airpods not in ear canal
<ul>
<li>better as can hear self singing</li>
</ul>
</li>
<li>beatsx in ear canal
<ul>
<li>blocks outside noise but also conducts via bone so self singing is heard roudry but distorted</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>05202</h2>
<ul>
<li>earmuffs
<ul>
<li>3m x5a 37db
<ul>
<li><a href="https://www.amazon.co.uk/gp/aw/d/B00BBCTQK6/ref=pd_aw_sims_1?pi=SL500_SY115">amazon</a> £26</li>
<li>very bulky but best isolation</li>
</ul>
</li>
<li>3m x4a 33db
<ul>
<li><a href="https://www.amazon.co.uk/gp/aw/d/B00BBCTQHY/ref=mp_s_a_1_17?qid=1464332438&amp;sr=8-17&amp;pi=AC_SX236_SY340_QL65&amp;keywords=ear+defenders">amazon</a>  £22
<ul>
<li>more stylish and less bulky than x5a but less isolation</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>05302</h2>
<ul>
<li>
<p><em>a longitudinal study of the process of acquiring absolute pitch: a practical report of training with the ‘chord identification method’</em><br />
ayako sakakibara 2014<br />
<a href="https://doi.org/10.1177%2F0305735612463948">doi.org/10.1177%2F0305735612463948</a></p>
<ul>
<li>The purpose of this study was to investigate longitudinally the process of acquiring absolute pitch (AP). Twenty-four young children (aged 2 to 6 years) without AP were trained to acquire AP using Eguchi’s (1991) Chord Identification Method (CIM). All children were able to acquire AP (except two who ceased training). Results suggest that, at a minimum, children younger than 6 years old are capable of acquiring AP through intentional training. Furthermore, children’s errors observed during training suggested the transition of different strategies relying respectively on tone height and tone chroma. Initially, children identified chords using a strategy depending primarily on tone height, then gradually they began to attend to tone chroma to identify chords and this process ultimately led to acquisition of AP.</li>
</ul>
</li>
<li>
<p>anki deck for pitch training (adults)<br />
single notes, one octave<br />
<a href="https://dl3.ankiweb.net/shared/downloadDeck2/789434294?k=Wzc4OTQzNDI5NCwgMjM3NzMsIDczNjI3N10.lPQ6XP3n5iB_9kt-XoHUCVmaXnzz2aWrCixSmrTkUUk">ankiweb</a></p>
<ul>
<li>consider making one according to the notes in the song “how far i’ll go” from <em>moana</em>, because teo knows the song well</li>
<li>make one for self with initial/important notes of favourite songs</li>
</ul>
</li>
</ul>
<h2>05402</h2>
<ul>
<li>
<p><em>polyphonic overtone singing</em><br />
anna-maria hefele et al. 2019<br />
<a href="http://youtube.com/watch?v=vc9qh709gas">youtube</a></p>
<ul>
<li>she made some tutorial videos which are helpful</li>
</ul>
</li>
</ul>
<h2>05502</h2>
<h2>05602</h2>
<h2>05702</h2>
<ul>
<li>
<p><em>ultra-open acoustic metamaterial silencer based on fano-like interference</em><br />
reza ghaffarivardavagh et al. 2019<br />
<a href="http://doi.org/10.1103/physrevb.99.024302">doi.org/10.1103/physrevb.99.024302</a></p>
<ul>
<li>
<p>Ghaffarivardavagh and Zhang let mathematics -- a shared passion that has buoyed both of their engineering careers and made them well-suited research partners -- guide them toward a workable design for what the acoustic metamaterial would look like.</p>
<p>They calculated the dimensions and specifications that the metamaterial would need to have in order to interfere with the transmitted sound waves, preventing sound -- but not air -- from being radiated through the open structure. The basic premise is that the metamaterial needs to be shaped in such a way that it sends incoming sounds back to where they came from, they say.</p>
<p>As a test case, they decided to create a structure that could silence sound from a loudspeaker. Based on their calculations, they modeled the physical dimensions that would most effectively silence noises. Bringing those models to life, they used 3D printing to materialize an open, noise-canceling structure made of plastic.</p>
<p>Trying it out in the lab, the researchers sealed the loudspeaker into one end of a PVC pipe. On the other end, the tailor-made acoustic metamaterial was fastened into the opening. With the hit of the play button, the experimental loudspeaker set-up came oh-so-quietly to life in the lab. Standing in the room, based on your sense of hearing alone, you'd never know that the loudspeaker was blasting an irritatingly high-pitched note. If, however, you peered into the PVC pipe, you would see the loudspeaker's subwoofers thrumming away.</p>
<p>The metamaterial, ringing around the internal perimeter of the pipe's mouth, worked like a mute button incarnate until the moment when Ghaffarivardavagh reached down and pulled it free. The lab suddenly echoed with the screeching of the loudspeaker's tune.</p>
<p>&quot;The moment we first placed and removed the silencer...was literally night and day,&quot; says Jacob Nikolajczyk, who in addition to being a study co author and former undergraduate researcher in Zhang's lab is a passionate vocal performer. &quot;We had been seeing these sorts of results in our computer modeling for months -- but it is one thing to see modeled sound pressure levels on a computer, and another to hear its impact yourself.&quot;</p>
<p>By comparing sound levels with and without the metamaterial fastened in place, the team found that they could silence nearly all -- 94 percent to be exact -- of the noise, making the sounds emanating from the loudspeaker imperceptible to the human ear.</p>
<p>Now that their prototype has proved so effective, the researchers have some big ideas about how their acoustic-silencing metamaterial could go to work making the real world quieter.</p>
<p>&quot;Drones are a very hot topic,&quot; Zhang says. Companies like Amazon are interested in using drones to deliver goods, she says, and &quot;people are complaining about the potential noise.&quot;</p>
<p>&quot;The culprit is the upward-moving fan motion,&quot; Ghaffarivardavagh says. &quot;If we can put sound-silencing open structures beneath the drone fans, we can cancel out the sound radiating toward the ground.&quot;</p>
<p>Closer to home -- or the office -- fans and HVAC systems could benefit from acoustic metamaterials that render them silent yet still enable hot or cold air to be circulated unencumbered throughout a building.</p>
<p>Ghaffarivardavagh and Zhang also point to the unsightliness of the sound barriers used today to reduce noise pollution from traffic and see room for an aesthetic upgrade. &quot;Our structure is super lightweight, open, and beautiful. Each piece could be used as a tile or brick to scale up and build a sound-canceling, permeable wall,&quot; they say.</p>
<p>The shape of acoustic-silencing metamaterials, based on their method, is also completely customizable, Ghaffarivardavagh says. The outer part doesn't need to be a round ring shape in order to function.</p>
<p>&quot;We can design the outer shape as a cube or hexagon, anything really,&quot; he says. &quot;When we want to create a wall, we will go to a hexagonal shape&quot; that can fit together like an open-air honeycomb structure.</p>
<p>Such walls could help contain many types of noises. Even those from the intense vibrations of an MRI machine, Zhang says.</p>
<p>According to Stephan Anderson, a professor of radiology at BU School of Medicine and a coauthor of the study, the acoustic metamaterial could potentially be scaled &quot;to fit inside the central bore of an MRI machine,&quot; shielding patients from the sound during the imaging process.</p>
<p>Zhang says the possibilities are endless, since the noise mitigation method can be customized to suit nearly any environment: &quot;The idea is that we can now mathematically design an object that can block the sounds of anything,&quot; she says</p>
</li>
<li>
<p><em>abstract</em> Recently, with advances in acoustic metamaterial science, the possibility of sound attenuation using subwavelength structures, while maintaining permeability to air, has been demonstrated. However, the ongoing challenge addressed herein is the fact that among such air-permeable structures to date, the open area represents only small fraction of the overall area of the material. In the presented paper in order to address this challenge, we first demonstrate that a transversely placed bilayer medium with large degrees of contrast in the layers' acoustic properties exhibits an asymmetric transmission, similar to the Fano-like interference phenomenon. Next, we utilize this design methodology and propose a deep-subwavelength acoustic metasurface unit cell comprising nearly 60% open area for air passage, while serving as a high-performance selective sound silencer. Finally, the proposed unit-cell performance is validated experimentally, demonstrating a reduction in the transmitted acoustic energy of up to 94%. This ultra-open metamaterial design, leveraging a Fano-like interference, enables high-performance sound silencing in a design featuring a large degree of open area, which may find utility in applications in which highly efficient, air-permeable sound silencers are required, such as smart sound barriers, fan or engine noise reduction, among others.</p>
</li>
</ul>
</li>
</ul>
<h2>05802</h2>
<h2>05902</h2>
<ul>
<li><em>old-school news: sensational, moralistic and, above all, sung</em><br />
una mcilvenna 2017<br />
<a href="https://aeon.co/ideas/old-school-news-sensational-moralistic-and-above-all-sung">aeon.co/ideas/old-school-news-sensational-moralistic-and-above-all-sung</a></li>
</ul>
<h2>06002</h2>
<h2>06102</h2>
<ul>
<li>
<p><em>population rate-coding predicts correctly that human sound localization depends on sound intensity</em><br />
antje ihlefeld et al. 2019<br />
<a href="http://doi.org/10.7554/elife.47027">doi.org/10.7554/elife.47027</a></p>
<ul>
<li>
<p>Unlike other sensory perceptions, such as feeling where raindrops hit the skin or being able to distinguish high notes from low on the piano, the direction of sounds must be computed; the brain estimates them by processing the difference in arrival time across the two ears, the so-called interaural time difference (ITD). A longstanding consensus among biomedical engineers is that humans localize sounds with a scheme akin to a spatial map or compass, with neurons aligned from left to right that fire individually when activated by a sound coming from a given angle -- say, at 30 degrees leftward from the center of the head.</p>
<p>But in research published this month in the journal eLife, Antje Ihlefeld, director of NJIT's Neural Engineering for Speech and Hearing Laboratory, is proposing a different model based on a more dynamic neural code. The discovery offers new hope, she says, that engineers may one day devise hearing aids, now notoriously poor in restoring sound direction, to correct this deficit.</p>
<p>&quot;If there is a static map in the brain that degrades and can't be fixed, that presents a daunting hurdle. It means people likely can't &quot;relearn&quot; to localize sounds well. But if this perceptual capability is based on a dynamic neural code, it gives us more hope of retraining peoples' brains,&quot; Ihlefeld notes. &quot;We would program hearing aids and cochlear implants not just to compensate for an individual's hearing loss, but also based upon how well that person could adapt to using cues from their devices. This is particularly important for situations with background sound, where no hearing device can currently restore the ability to single out the target sound. We know that providing cues to restore sound direction would really help.&quot;</p>
<p>What led her to this conclusion is a journey of scholarly detective work that began with a conversation with Robert Shapley, an eminent neurophysiologist at NYU who remarked on a peculiarity of human binocular depth perception -- the ability to determine how far away a visual object is -- that also depends on a computation comparing input received by both eyes. Shapley noted that these distance estimates are systematically less accurate for low-contrast stimuli (images that are more difficult to distinguish from their surrounding) than for high-contrast ones.</p>
<p>Ihlefeld and Shapley wondered if the same neural principle applied to sound localization: whether it is less accurate for softer sounds than for louder ones. But this would depart from the prevailing spatial map theory, known as the Jeffress model, which holds that sounds of all volumes are processed -- and therefore perceived -- the same way. Physiologists, who propose that mammals rely on a more dynamic neural model, have long disagreed with it. They hold that mammalian neurons tend to fire at different rates depending on directional signals and that the brain then compares these rates across sets of neurons to dynamically build up a map of the sound environment.</p>
<p>&quot;The challenge in proving or disproving these theories is that we can't look directly at the neural code for these perceptions because the relevant neurons are located in the human brainstem, so we cannot obtain high-resolution images of them,&quot; she says. &quot;But we had a hunch that the two models would give different sound location predictions at a very low volume.&quot;</p>
<p>They searched the literature for evidence and found only two papers that had recorded from neural tissue at these low sounds. One study was in barn owls -- a species thought to rely on the Jeffress model, based on high-resolution recordings in the birds' brain tissue -- and the other study was in a mammal, the rhesus macaque, an animal thought to use dynamic rate coding. They then carefully reconstructed the firing properties of the neurons recorded in these old studies and used their reconstructions to estimate sound direction both as a function of ITD and volume.</p>
<p>&quot;We expected that for the barn owl data, it really should not matter how loud a source is -- the predicted sound direction should be really accurate no matter the sound volume -- and we were able to confirm that. However, what we found for the monkey data is that predicted sound direction depended on both ITD and volume,&quot; she said. &quot;We then searched the human literature for studies on perceived sound direction as a function of ITD, which was also thought not to depend on volume, but surprisingly found no evidence to back up this long-held belief.&quot;</p>
<p>She and her graduate student, Nima Alamatsaz, then enlisted volunteers on the NJIT campus to test their hypothesis, using sounds to test how volume affects where people think a sound emerges.</p>
<p>&quot;We built an extremely quiet, sound-shielded room with specialized calibrated equipment that allowed us to present sounds with high precision to our volunteers and record where they perceived the sound to originate. And sure enough, people misidentified the softer sounds,&quot; notes Alamatsaz.</p>
<p>&quot;To date, we are unable to describe sound localization computations in the brain precisely,&quot; adds Ihlefeld. &quot;However, the current results are inconsistent with the notion that the human brain relies on a Jeffress-like computation. Instead, we seem to rely on a slightly less accurate mechanism.</p>
<p>More broadly, the researchers say, their studies point to direct parallels in hearing and visual perception that have been overlooked before now and that suggest that rate-based coding is a basic underlying operation when computing spatial dimensions from two sensory inputs.</p>
<p>&quot;Because our work discovers unifying principles across the two senses, we anticipate that interested audiences will include cognitive scientists, physiologists and computational modeling experts in both hearing and vision,&quot; Ihlefeld says. &quot;It is fascinating to compare how the brain uses the information reaching our eyes and ears to make sense of the world around us and to discover that two seemingly unconnected perceptions -- vision and hearing -- may in fact be quite similar after all.&quot;</p>
</li>
<li>
<p><em>abstract</em> Human sound localization is an important computation performed by the brain. Models of sound localization commonly assume that sound lateralization from interaural time differences is level invariant. Here we observe that two prevalent theories of sound localization make opposing predictions. The labelled-line model encodes location through tuned representations of spatial location and predicts that perceived direction is level invariant. In contrast, the hemispheric-difference model encodes location through spike-rate and predicts that perceived direction becomes medially biased at low sound levels. Here, behavioral experiments find that softer sounds are perceived closer to midline than louder sounds, favoring rate-coding models of human sound localization. Analogously, visual depth perception, which is based on interocular disparity, depends on the contrast of the target. The similar results in hearing and vision suggest that the brain may use a canonical computation of location: encoding perceived location through population spike rate relative to baseline.</p>
</li>
</ul>
</li>
</ul>
<h2>06202</h2>
<h2>06302</h2>
<h2>06402</h2>
<h2>06502</h2>
<h2>06602</h2>
<h2>06702</h2>
<h2>06802</h2>
<h2>06902</h2>
<h2>07002</h2>
<h2>07102</h2>
<h2>07202</h2>
<h2>07302</h2>
<h2>07402</h2>
<h2>07502</h2>
<h2>07602</h2>
<h2>07702</h2>
<h2>07802</h2>
<h2>07902</h2>
<ul>
<li>
<p><em>musical instruments as sensors</em><br />
heran c. bhakta et al. 2018<br />
<a href="http://doi.org/10.1021/acsomega.8b01673">doi.org/10.1021/acsomega.8b01673</a></p>
<ul>
<li>The frequencies of notes made by a musical instrument are determined by the physical properties of the instrument. Consequently, by measuring the frequency of a note, one can infer information about the instrument’s physical properties. In this work, we show that by modifying a musical instrument to contain a sample and analyzing the instrument’s pitch, we can make precision measurements of the physical properties of the sample. We used the mbira, a 3000-year-old African musical instrument that consists of metal tines attached to a wooden board; these tines are plucked to play musical notes. By replacing the mbira’s tines with bent steel tubing, filling the tubing with a sample, using a smartphone to record the sound while plucking the tubing, and measuring the frequency of the sound using a free software tool on our website, we can measure the density of the sample with a resolution of about 0.012 g/mL. Unlike existing tools for measuring density, the mbira sensor can be made and used by virtually anyone in the world. To demonstrate the mbira sensor’s capabilities, we used it to successfully distinguish diethylene glycol and glycerol, two similar chemicals that are sometimes mistaken for each other in pharmaceutical manufacturing (leading to hundreds of deaths). We also show that consumers could use mbira sensors to detect counterfeit and adulterated medications (which represent around 10% of all medications in low- and middle-income countries). We expect that many other musical instruments can function as sensors and find important and lifesaving applications.</li>
</ul>
</li>
<li>
<p><em>a self-consistent sonification method to translate amino acid sequences into musical compositions and application in protein design using artificial intelligence</em><br />
chi-hua yu et al. 2019<br />
<a href="http://doi.org/10.1021/acsnano.9b02180">doi.org/10.1021/acsnano.9b02180</a></p>
<ul>
<li>
<p>a system for converting the molecular structures of proteins, the basic building blocks of all living beings, into audible sound that resembles musical passages. Then, reversing the process, they can introduce some variations into the music and convert it back into new proteins never before seen in nature.</p>
<p>Although it's not quite as simple as humming a new protein into existence, the new system comes close. It provides a systematic way of translating a protein's sequence of amino acids into a musical sequence, using the physical properties of the molecules to determine the sounds. Although the sounds are transposed in order to bring them within the audible range for humans, the tones and their relationships are based on the actual vibrational frequencies of each amino acid molecule itself, computed using theories from quantum chemistry.</p>
<p>The system was developed by Markus Buehler, the McAfee Professor of Engineering and head of the Department of Civil and Environmental Engineering at MIT, along with postdoc Chi Hua Yu and two others. As described in the journal ACS Nano, the system translates the 20 types of amino acids, the building blocks that join together in chains to form all proteins, into a 20-tone scale. Any protein's long sequence of amino acids then becomes a sequence of notes.</p>
<p>While such a scale sounds unfamiliar to people accustomed to Western musical traditions, listeners can readily recognize the relationships and differences after familiarizing themselves with the sounds. Buehler says that after listening to the resulting melodies, he is now able to distinguish certain amino acid sequences that correspond to proteins with specific structural functions. &quot;That's a beta sheet,&quot; he might say, or &quot;that's an alpha helix.&quot;</p>
<p>Learning the language of proteins</p>
<p>The whole concept, Buehler explains, is to get a better handle on understanding proteins and their vast array of variations. Proteins make up the structural material of skin, bone, and muscle, but are also enzymes, signaling chemicals, molecular switches, and a host of other functional materials that make up the machinery of all living things. But their structures, including the way they fold themselves into the shapes that often determine their functions, are exceedingly complicated. &quot;They have their own language, and we don't know how it works,&quot; he says. &quot;We don't know what makes a silk protein a silk protein or what patterns reflect the functions found in an enzyme. We don't know the code.&quot;</p>
<p>By translating that language into a different form that humans are particularly well-attuned to, and that allows different aspects of the information to be encoded in different dimensions -- pitch, volume, and duration -- Buehler and his team hope to glean new insights into the relationships and differences between different families of proteins and their variations, and use this as a way of exploring the many possible tweaks and modifications of their structure and function. As with music, the structure of proteins is hierarchical, with different levels of structure at different scales of length or time.</p>
<p>The team then used an artificial intelligence system to study the catalog of melodies produced by a wide variety of different proteins. They had the AI system introduce slight changes in the musical sequence or create completely new sequences, and then translated the sounds back into proteins that correspond to the modified or newly designed versions. With this process they were able to create variations of existing proteins -- for example of one found in spider silk, one of nature's strongest materials -- thus making new proteins unlike any produced by evolution.</p>
<p>Although the researchers themselves may not know the underlying rules, &quot;the AI has learned the language of how proteins are designed,&quot; and it can encode it to create variations of existing versions, or completely new protein designs, Buehler says. Given that there are &quot;trillions and trillions&quot; of potential combinations, he says, when it comes to creating new proteins &quot;you wouldn't be able to do it from scratch, but that's what the AI can do.&quot;</p>
<p>&quot;Composing&quot; new proteins</p>
<p>By using such a system, he says training the AI system with a set of data for a particular class of proteins might take a few days, but it can then produce a design for a new variant within microseconds. &quot;No other method comes close,&quot; he says. &quot;The shortcoming is the model doesn't tell us what's really going on inside. We just know it works.&quot;</p>
<p>This way of encoding structure into music does reflect a deeper reality. &quot;When you look at a molecule in a textbook, it's static,&quot; Buehler says. &quot;But it's not static at all. It's moving and vibrating. Every bit of matter is a set of vibrations. And we can use this concept as a way of describing matter.&quot;</p>
<p>The method does not yet allow for any kind of directed modifications -- any changes in properties such as mechanical strength, elasticity, or chemical reactivity will be essentially random. &quot;You still need to do the experiment,&quot; he says. When a new protein variant is produced, &quot;there's no way to predict what it will do.&quot;</p>
<p>The team also created musical compositions developed from the sounds of amino acids, which define this new 20-tone musical scale. The art pieces they constructed consist entirely of the sounds generated from amino acids. &quot;There are no synthetic or natural instruments used, showing how this new source of sounds can be utilized as a creative platform,&quot; Buehler says. Musical motifs derived from both naturally existing proteins and AI-generated proteins are used throughout the examples, and all the sounds, including some that resemble bass or snare drums, are also generated from the sounds of amino acids.</p>
<p>The researchers have created a free Android smartphone app, called Amino Acid Synthesizer, to play the sounds of amino acids and record protein sequences as musical compositions.</p>
</li>
<li>
<p><em>abstract</em> We report a self-consistent method to translate amino acid sequences into audible sound, use the representation in the musical space to train a neural network, and then apply it to generate protein designs using artificial intelligence (AI). The sonification method proposed here uses the normal mode vibrations of the amino acid building blocks of proteins to compute an audible representation of each of the 20 natural amino acids, which is fully defined by the overlay of its respective natural vibrations. The vibrational frequencies are transposed to the audible spectrum following the musical concept of transpositional equivalence, playing or writing music in a way that makes it sound higher or lower in pitch while retaining the relationships between tones or chords played. This transposition method ensures that the relative values of the vibrational frequencies within each amino acid and among different amino acids are retained. The characteristic frequency spectrum and sound associated with each of the amino acids represents a type of musical scale that consists of 20 tones, the “amino acid scale”. To create a playable instrument, each tone associated with the amino acids is assigned to a specific key on a piano roll, which allows us to map the sequence of amino acids in proteins into a musical score. To reflect higher-order structural details of proteins, the volume and duration of the notes associated with each amino acid are defined by the secondary structure of proteins, computed using DSSP and thereby introducing musical rhythm. We then train a recurrent neural network based on a large set of musical scores generated by this sonification method and use AI to generate musical compositions, capturing the innate relationships between amino acid sequence and protein structure. We then translate the de novo musical data generated by AI into protein sequences, thereby obtaining de novo protein designs that feature specific design characteristics. We illustrate the approach in several examples that reflect the sonification of protein sequences, including multihour audible representations of natural proteins and protein-based musical compositions solely generated by AI. The approach proposed here may provide an avenue for understanding sequence patterns, variations, and mutations and offers an outreach mechanism to explain the significance of protein sequences. The method may also offer insight into protein folding and understanding the context of the amino acid sequence in defining the secondary and higher-order folded structure of proteins and could hence be used to detect the effects of mutations through sound.</p>
</li>
</ul>
</li>
</ul>
<h2>08002</h2>
<ul>
<li>
<p><em>the sound produced by a dripping tap is driven by resonant oscillations of an entrapped air bubble</em><br />
samuel phillips et al. 2018<br />
<a href="http://doi.org/10.1038/s41598-018-27913-0">doi.org/10.1038/s41598-018-27913-0</a></p>
<ul>
<li>A similar result was achieved using the second method, with washing-up liquid used as the surfactant. A video of the resulting drop impact is available as Supplementary Video S4. Bubble entrainment was prevented and no sound was produced, again confirming that the bubble-entrainment mechanism underpins the characteristic ‘plink’ sound that is of interest. Interestingly, this second result also suggests that small changes in surface tension have an effect on bubble entrainment and so this addition of a small amount of washing up liquid to the body of water may represent a simple and novel way to alleviate the annoying noise of a leaking tap or water dripping from a roof into a bucket.</li>
</ul>
</li>
<li>
<p><em>photosynthesis by marine algae produces sound, contributing to the daytime soundscape on coral reefs</em><br />
simon e. freeman et al. 2018<br />
<a href="http://doi.org/10.1371/journal.pone.0201766">doi.org/10.1371/journal.pone.0201766</a></p>
<ul>
<li>We have observed that marine macroalgae produce sound during photosynthesis. The resultant soundscapes correlate with benthic macroalgal cover across shallow Hawaiian coral reefs during the day, despite the presence of other biological noise. Likely ubiquitous but previously overlooked, this source of ambient biological noise in the coastal ocean is driven by local supersaturation of oxygen near the surface of macroalgal filaments, and the resultant formation and release of oxygen-containing bubbles into the water column. During release, relaxation of the bubble to a spherical shape creates a monopole sound source that ‘rings’ at the Minnaert frequency. Many such bubbles create a large, distributed sound source over the sea floor. Reef soundscapes contain vast quantities of biological information, making passive acoustic ecosystem evaluation a tantalizing prospect if the sources are known. Our observations introduce the possibility of a general, volumetrically integrative, noninvasive, rapid and remote technique for evaluating algal abundance and rates of primary productivity in littoral aquatic communities. Increased algal cover is one of the strongest indicators for coral reef ecosystem stress. Visually determining variations in algal abundance is a time-consuming and expensive process. This technique could therefore provide a valuable tool for ecosystem management but also for industrial monitoring of primary production, such as in algae-based biofuel synthesis.</li>
</ul>
</li>
<li>
<p><em>near-surface environmentally forced changes in the ross ice shelf observed with ambient seismic noise</em><br />
j. chaput et al. 2018<br />
<a href="http://doi.org/10.1029/2018GL079665">doi.org/10.1029/2018GL079665</a></p>
<ul>
<li>Continuous seismic observations across the Ross Ice Shelf reveal ubiquitous ambient resonances at frequencies &gt;5 Hz. These firn‐trapped surface wave signals arise through wind and snow bedform interactions coupled with very low velocity structures. Progressive and long‐term spectral changes are associated with surface snow redistribution by wind and with a January 2016 regional melt event. Modeling demonstrates high spectral sensitivity to near‐surface (top several meters) elastic parameters. We propose that spectral peak changes arise from surface snow redistribution in wind events and to velocity drops reflecting snow lattice weakening near 0°C for the melt event. Percolation‐related refrozen layers and layer thinning may also contribute to long‐term spectral changes after the melt event. Single‐station observations are inverted for elastic structure for multiple stations across the ice shelf. High‐frequency ambient noise seismology presents opportunities for continuous assessment of near‐surface ice shelf or other firn environments.</li>
</ul>
</li>
</ul>
<h2>08102</h2>
<h2>08202</h2>
<h2>08302</h2>
<ul>
<li>video</li>
<li>range of formats / subtitles
<ul>
<li>
<p>readdle <em>documents</em> (ios)<br />
Movie viewer: .3gp, .l16, .m3u, .m4v, .mm, .mov, .mp4, .scm, .avi, .mkv, .flv; </p>
</li>
<li>
<p>vlc<br />
<a href="https://videolan.org/vlc/">videolan.org/vlc/</a></p>
<ul>
<li>copes better than documents with some video files</li>
</ul>
</li>
</ul>
</li>
<li>archive
<ul>
<li>
<p>youtube</p>
<ul>
<li>
<p>media grabber<br />
<a href="https://github.com/mediagrabber/ios-workflow/blob/master/media%20grabber.wflow">https://github.com/mediagrabber/ios-workflow/blob/master/media%20grabber.wflow</a></p>
</li>
<li>
<p>yet another youtube video/music download shortcut<br />
by<br />
<a href="https://www.reddit.com/user/arachno7">u/arachno7</a></p>
<p><a href="https://www.reddit.com/r/shortcuts/comments/9pqkfo/yet_another_youtube_videomusic_download_shortcut/">https://www.reddit.com/r/shortcuts/comments/9pqkfo/yet_another_youtube_videomusic_download_shortcut/</a></p>
</li>
<li>
<p>ultimate downloader<br />
<a href="https://routinehub.co/shortcut/368">https://routinehub.co/shortcut/368</a></p>
</li>
</ul>
</li>
<li>
<p>infuse 4</p>
<ul>
<li>can play mkv without stuttering<br />
but new infuse 5 too expensive</li>
</ul>
</li>
<li>
<p>download youtube videos</p>
<ul>
<li>
<p>icab mobile can download some videos<br />
long press on the video</p>
</li>
<li>
<p>using <em>workflow</em> ios app<br />
<a href="https://www.reddit.com/r/workflow/comments/6rj4t3/workflow_to_download_youtube_videos_in_ios/">reddit thread</a></p>
<p><a href="https://workflow.is/workflows/adf45a1786ff40efb21f0a2765dae3ea">https://workflow.is/workflows/adf45a1786ff40efb21f0a2765dae3ea</a></p>
</li>
<li>
<p>firefox (mac)<br />
Link: <a href="https://www.mozilla.org/en-US/firefox">mozilla.org/en-US/firefox</a></p>
<ul>
<li>youtube extension</li>
</ul>
</li>
</ul>
</li>
<li>
<p>nplayer, splayer not tried</p>
</li>
<li>
<p>usb cable transfer (quick)</p>
<ul>
<li>
<p>goodreader usb<br />
<a href="http://www.goodreader.net/usb/">goodreader.net/usb/</a></p>
<ul>
<li>html5 video download<br />
Link: <a href="http://goodreader.com/gr-man-howto.html#html5video">goodreader.com/gr-man-howto.html#html5video</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>08402</h2>
<h2>08502</h2>
<ul>
<li>podcast</li>
<li><em>anchor</em> (untried)<br />
<a href="https://apps.apple.com/gb/app/anchor/id1056182234">apps.apple.com/gb/app/anchor/id1056182234</a></li>
</ul>
<h2>08602</h2>
<h2>08702</h2>
<h2>08802</h2>
<h2>08902</h2>
<h2>09002</h2>
<h2>09102</h2>
<ul>
<li>
<p>yamaha cp300 (old)</p>
</li>
<li>
<p>charge ipad 1 passthrough on irig midi</p>
<ul>
<li>short mini–usb cable seems to work best, turn off ipad while charging</li>
</ul>
</li>
<li>
<p>musescore.com<br />
<a href="http://www.musescore.com">musescore.com</a></p>
<ul>
<li>download midi</li>
<li>play in synthesia ipad</li>
<li>connected to irig midi</li>
<li>connected to yamaha cp300</li>
</ul>
</li>
</ul>
<h2>09202</h2>
<ul>
<li>music apps
<ul>
<li>
<p><em>crypt of the necrodancer amplified</em><br />
<a href="https://apps.apple.com/gb/app/necrodancer-amplified/id1445623416">apps.apple.com/gb/app/necrodancer-amplified/id1445623416</a></p>
</li>
<li>
<p>archive</p>
<ul>
<li>
<p>mimi (ios)<br />
Link: <a href="https://itunes.apple.com/us/app/mimi-music-sound-made-for/id1055611099?mt=8">itunes.apple.com/us/app/mimi-music-sound-made-for/id1055611099</a></p>
</li>
<li>
<p>caesium (ios)<br />
Link: <a href="https://itunes.apple.com/gb/app/cesium-music-player/id924491991?mt=8">itunes.apple.com/gb/app/cesium-music-player/id924491991</a></p>
</li>
<li>
<p>midi</p>
<p><a href="http://sourceforge.net/projects/midisheetmusic/">23</a> free–mac download a midi file and play it with just the computer while displaying sheet music. if you want to play the midi through to the piano, then can download the free version of synthesia and it will do it for you.</p>
<p><a href="http://ariamaestosa.sourceforge.net">aria maestosa</a> and <a href="http://musescore.org">musescore</a> free–mac midi players and editors</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>09302</h2>
<ul>
<li>
<p>AudioKit free apps<br />
<a href="https://audiokitpro.com">audiokitpro.com</a></p>
<ul>
<li>
<p><em>synth one</em><br />
<a href="https://itunes.apple.com/gb/app/audiokit-synth-one-synthesizer/id1371050497?mt=8">itunes.apple.com/gb/app/audiokit-synth-one-synthesizer/id1371050497</a></p>
</li>
<li>
<p><em>FM player: classic dx synths</em><br />
<a href="https://itunes.apple.com/gb/app/fm-player-classic-dx-synths/id1307785646?mt=8">itunes.apple.com/gb/app/fm-player-classic-dx-synths/id1307785646</a></p>
</li>
</ul>
</li>
</ul>
<h2>09402</h2>
<ul>
<li>audio file tag editor
<ul>
<li>kid3 <a href="https://kid3.sourceforge.io">kid3.sourceforge.io</a></li>
</ul>
</li>
</ul>
<h2>09502</h2>
<ul>
<li>
<p>song recording, editing</p>
</li>
<li>
<p><em>garageband</em></p>
<ul>
<li>make ringtone<br />
Link: <a href="http://www.macworld.com/article/2079194/how-to-create-ios-ringtones.html">macworld.com/article/2079194/how-to-create-ios-ringtones.html</a></li>
</ul>
</li>
<li>
<p><em>logic pro</em> (mac)</p>
<ul>
<li>
<p>make ringtone<br />
Link: <a href="https://www.youtube.com/watch?v=lzq2SrDnP1E">youtube.com/watch</a></p>
</li>
<li>
<p>logic remote manual<br />
<a href="https://help.apple.com/logicremote/ipad/1.0/#">help.apple.com/logicremote/ipad/1.0/#</a></p>
</li>
</ul>
</li>
<li>
<p><em>audacity</em> (mac)<br />
<a href="http://web.audacityteam.org/">web.audacityteam.org/</a></p>
</li>
</ul>
<h2>09602</h2>
<ul>
<li>
<p>apps not yet tried</p>
</li>
<li>
<p>airfoil<br />
<a href="https://rogueamoeba.com/airfoil/mac/">rogueamoeba.com/airfoil/mac/</a></p>
<ul>
<li>can broadcast audio from ios to multiple devices</li>
</ul>
</li>
<li>
<p>airserver</p>
<ul>
<li>randomly, this works better for me when the ipad pro was connected via a usb cable to the mac, otherwise it was unacceptably laggy</li>
</ul>
</li>
<li>
<p>mirror ios to mac using quicktime and lightning cable</p>
<ul>
<li>Connect your device to your mac using your Lightning to USB cable.<br />
2. Open Quicktime on your mac<br />
3. Click on File in the menu bar at the top of the screen.<br />
4. Scroll down to “New Movie Recording.”<br />
5. Next to the red record button (bottom middle), click on the down arrow.</li>
</ul>
</li>
<li>
<p>just press record<br />
<a href="https://itunes.apple.com/gb/app/just-press-record/id1033342465?mt=8">itunes.apple.com/gb/app/just-press-record/id1033342465?mt=8</a></p>
</li>
<li>
<p><em>auphonic</em> automatic audio post production web service<br />
<a href="https://auphonic.com/pricing">auphonic.com/pricing</a></p>
</li>
<li>
<p><em>ferrite</em> for recording audio and editing on ios<br />
<a href="http://www.wooji-juice.com/products/ferrite/">wooji-juice.com/products/ferrite/</a></p>
<ul>
<li>
<p>tutorials</p>
<p><a href="https://www.youtube.com/user/woojijuice">youtube.com/user/woojijuice</a></p>
</li>
<li>
<p>customisable keyboard shortcuts</p>
</li>
</ul>
</li>
<li>
<p>recording podcasts howto<br />
<a href="https://youtu.be/0K4M7dy1Ehg">youtu.be/0K4M7dy1Ehg</a></p>
</li>
</ul>
<h2>09702</h2>
<ul>
<li>transfer music files from ios <em>to</em> mac
<ul>
<li>
<p><em>imazing</em><br />
<a href="https://imazing.com/">imazing.com/</a></p>
</li>
<li>
<p>education discount<br />
<a href="https://imazing.com/store/educational">imazing.com/store/educational</a></p>
</li>
<li>
<p>can transfer some but not most video formats</p>
</li>
</ul>
</li>
</ul>
<h2>09802</h2>
<ul>
<li>not yet read
<ul>
<li>
<p><em>anatomy of the voice: an illustrated guide for singers, vocal coaches, and speech therapists</em><br />
theodore dimon 2018</p>
</li>
<li>
<p><em>why you love music: from mozart to metallica—the emotional power of beautiful sounds</em><br />
john powell 2017</p>
</li>
<li>
<p><em>sound: a story of hearing lost and found</em><br />
bella bathurst 2018</p>
</li>
<li>
<p><em>the music instinct: how music works and why we can’t do without it</em><br />
philip ball 2010</p>
</li>
<li>
<p><em>the sound book: the science of the sonic wonders of the world</em><br />
trevor cox 2014</p>
</li>
<li>
<p><em>i can hear you whisper: an intimate journey through the science of sound and language</em><br />
lydia denworth 2014</p>
</li>
<li>
<p><em>principles of violin playing and teaching</em><br />
ivan galamian 1962</p>
</li>
</ul>
</li>
</ul>
<h2>09902</h2>
<ul>
<li>
<p>speakers</p>
<ul>
<li>stuff and things pa mk4 £1000!<br />
<a href="http://www.stuffandthings.co.uk/pa_mk4.htm">stuffandthings.co.uk/pa_mk4.htm</a></li>
</ul>
</li>
<li>
<p>busking<br />
<a href="https://busk.co/blog/">busk.co/blog/</a></p>
</li>
</ul>
<h2>04902</h2>
<h2>04802</h2>
<h2>04702</h2>
<h2>04602</h2>
<ul>
<li>song lyrics
<ul>
<li>let me understand my father’s years</li>
</ul>
</li>
</ul>
<h2>04502</h2>
<ul>
<li>bass line
<ul>
<li>20150130
<ul>
<li>a two octaves below middle c</li>
</ul>
</li>
<li>a g f e</li>
</ul>
</li>
</ul>
<h2>04402</h2>
<ul>
<li>high riff
<ul>
<li>20150118
<ul>
<li>d two octaves above middle c</li>
</ul>
</li>
<li>dfafafcfcfcf</li>
</ul>
</li>
</ul>
<h2>04302</h2>
<h2>04202</h2>
<h2>04102</h2>
<h2>04002</h2>
<h2>03902</h2>
<h2>03802</h2>
<h2>03702</h2>
<h2>03602</h2>
<h2>03502</h2>
<ul>
<li>magic knight rayearth
<ul>
<li>e e b
<ul>
<li>tomaranai</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>03402</h2>
<h2>03302</h2>
<h2>03202</h2>
<ul>
<li>final countdown “it’s the final countdown” to rocky theme “dadadadada” (not eye of tiger)</li>
</ul>
<h2>03102</h2>
<h2>03002</h2>
<h2>02902</h2>
<ul>
<li>it's not that i can hear better than others, it is just a skill that comes more easily with practice, like other unusual skills like stereograms or singing without using lips</li>
</ul>
<h2>02802</h2>
<h2>02702</h2>
<h2>02602</h2>
<ul>
<li>can whistle both when exhaling and when inhaling, though clearly not as well; can enunciate clearly using only tongue and teeth, not requiring lips. can I learn to sing when inhaling? is it even physiologically possible?
<ul>
<li>update 20171003: yes it is! not sure if it is bad for you though…</li>
</ul>
</li>
</ul>
<h2>02502</h2>
<h2>02402</h2>
<h2>02302</h2>
<h2>02202</h2>
<h2>02102</h2>
<h2>02002</h2>
<h2>01902</h2>
<h2>01802</h2>
<ul>
<li>am not less susceptible to visual bias — it is just that I do not rely on my vision as much as others do, since I am visually impaired. am also not less susceptible to auditory bias, but I do rely on hearing more than most others do, so the chance of auditory biases affecting me is higher</li>
</ul>
<h2>01702</h2>
<h2>01602</h2>
<h2>01502</h2>
<h2>01402</h2>
<h2>01302</h2>
<h2>01202</h2>
<ul>
<li>
<p>for me, practicing and playing piano is like meditation or yoga or running — serenely in the moment</p>
</li>
<li>
<p>how I learn piano pieces<br />
i trust that i will learn the piece, even if it seems daunting at first; i know that with spaced recall and starting from the end, i will eventually learn to play it from recall and by touch</p>
</li>
<li>
<p>learning a piano piece by spaced recall of music notation (traditional notation or synthesia). goal is final ability to perform piece by touch and sound, with neither notation or vision</p>
<ul>
<li>learn individual notes
<ul>
<li>see or hear piece played by past master, repeat listening regularly as desired</li>
<li>learn individual bars of piece, order of learning is each bar from end of piece forwards — in actual practice, play the section of the piece from the most recently learned bar through to the end, once through, each morning and night — no more than this, as time better spent elsewhere otherwise</li>
<li>with each repetition and rest, recall develops, with the final bars being most easily recalled as they have been cycled most</li>
</ul>
</li>
<li>play to express
<ul>
<li>as we go through the process of learning the notes, the more easily recalled parts start to approach performance speed</li>
<li>as we recall the parts regularly, patterns become more salient in the written phrases and we can use those patterns to cue recall — “chunking”</li>
<li>the more easily recalled parts can now be played while looking only at the music notation, and we do not need to look at the placement of the fingers and hands, even for large transitions across the keyboard</li>
</ul>
</li>
<li>play to evoke
<ul>
<li>continued recall enables us to produce the piece at will without referring to the notation or looking at the keyboard — at this point I play “blind”</li>
<li>expression continues to develop as we feel more nuances in the piece — advanced “chunking”</li>
<li>as chunking develops, in learning new pieces you may become able to altogether bypass earlier chunking stages such as learning individual notes or using notation. this is essentially the same developmental process as reading words, learning vocals, dance movements — it is now a dance between the hands that happens to be at a keyboard</li>
</ul>
</li>
<li>from memory, by touch without sight, so i can sing at the same time. i learn by the sequential pattern that the notes make with one another rather than from visual interpretation from a page. similarly with reading, it is the pattern rather than the words themselves as words that makes sense to me.</li>
</ul>
</li>
</ul>
<h2>01102</h2>
<h2>01002</h2>
<ul>
<li>一番の宝物 (yui final version)<br />
ichibannotakaramono<br />
my most precious treasure<br />
lisa</li>
</ul>
<h2>00902</h2>
<ul>
<li>enunciate vowels using the back of the tongue and mouth</li>
</ul>
<h2>00802</h2>
<ul>
<li>
<p>enunciate consonants using just your tongue and the back of your top front teeth. it is possible to do this with all the consonants, even ‘b’</p>
</li>
<li>
<p><em>human sound systems are shaped by post-neolithic changes in bite configuration</em><br />
d. e. blasi et al. 2019<br />
<a href="http://doi.org/10.1126/science.aav3218">doi.org/10.1126/science.aav3218</a></p>
<p>they did not seem to consider that fricatives can be made just with upper teeth and tongue</p>
<ul>
<li>
<p>While the teeth of humans used to meet in an edge-to-edge bite due to their harder and tougher diet at the time, more recent softer foods allowed modern humans to retain the juvenile overbite that had previously disappeared by adulthood, with the upper teeth slightly more in front than the lower teeth. This shift led to the rise of a new class of speech sounds now found in half of the world's languages: labiodentals, or sounds made by touching the lower lip to the upper teeth, for example when pronouncing the letter &quot;f.&quot;</p>
<p>&quot;In Europe, our data suggests that the use of labiodentals has increased dramatically only in the last couple of millennia, correlated with the rise of food processing technology such as industrial milling,&quot; explains Steven Moran, one of the two co-first authors of the study. &quot;The influence of biological conditions on the development of sounds has so far been underestimated.&quot;</p>
<p>Interdisciplinary approach to verify hypothesis</p>
<p>The project was inspired by an observation made by linguist Charles Hockett back in 1985. Hockett noticed that languages that foster labiodentals are often found in societies with access to softer foods. &quot;But there are dozens of superficial correlations involving language which are spurious, and linguistic behavior, such as pronunciation, doesn't fossilize,&quot; says co-first author Damián Blasi.</p>
<p>In order to unravel the mechanisms underlying the observed correlations, the scientists combined insights, data and methods from across the sciences, including biological anthropology, phonetics and historical linguistics. &quot;It was a rare case of consilience across disciplines,&quot; says Blasi. What made the project possible was the availability of newly developed, large datasets, detailed biomechanical simulation models, and computationally intensive methods of data analysis, according to the researchers.</p>
<p>Listening in on the past</p>
<p>&quot;Our results shed light on complex causal links between cultural practices, human biology and language,&quot; says Balthasar Bickel, project leader and UZH professor. &quot;They also challenge the common assumption that, when it comes to language, the past sounds just like the present.&quot; Based on the findings of the study and the new methods it developed, linguists can now tackle a host of unsolved questions, such as how languages actually sounded thousands of years ago. Did Caesar say &quot;veni, vidi, vici&quot; -- or was it more like &quot;weni, widi, wici'&quot;?</p>
</li>
<li>
<p><em>abstract</em> INTRODUCTION<br />
Human speech manifests itself in spectacular diversity, ranging from ubiquitous sounds such as “m” and “a” to the rare click consonants in some languages of southern Africa. This range is generally thought to have been fixed by biological constraints since at least the emergence of Homo sapiens. At the same time, the abundance of each sound in the languages of the world is commonly taken to depend on how easy the sound is to produce, perceive, and learn. This dependency is also regarded as fixed at the species level.<br />
RATIONALE<br />
Given this dependency, we expect that any change in the human apparatus for production, perception, or learning affects the probability—or even the range—of the sounds that languages have. Paleoanthropological evidence suggests that the production apparatus has undergone a fundamental change of just this kind since the Neolithic. Although humans generally start out with vertical and horizontal overlap in their bite configuration (overbite and overjet, respectively), masticatory exertion in the Paleolithic gave rise to an edge-to-edge bite after adolescence. Preservation of overbite and overjet began to persist long into adulthood only with the softer diets that started to become prevalent in the wake of agriculture and intensified food processing. We hypothesize that this post-Neolithic decline of edge-to-edge bite enabled the innovation and spread of a new class of speech sounds that is now present in nearly half of the world’s languages: labiodentals, produced by positioning the lower lip against the upper teeth, such as in “f” or “v.”<br />
RESULTS<br />
Biomechanical models of the speech apparatus show that labiodentals incur about 30% less muscular effort in the overbite and overjet configuration than in the edge-to-edge bite configuration. This difference is not present in similar articulations that place the upper lip, instead of the teeth, against the lower lip (as in bilabial “m,” “w,” or “p”). Our models also show that the overbite and overjet configuration reduces the incidental tooth/lip distance in bilabial articulations to 24 to 70% of their original values, inviting accidental production of labiodentals. The joint effect of a decrease in muscular effort and an increase in accidental production predicts a higher probability of labiodentals in the language of populations where overbite and overjet persist into adulthood. When the persistence of overbite and overjet in a population is approximated by the prevalence of agriculturally produced food, we find that societies described as hunter-gatherers indeed have, on average, only about one-fourth the number of labiodentals exhibited by food-producing societies, after controlling for spatial and phylogenetic correlation. When the persistence is approximated by the increase in food-processing technology over the history of one well-researched language family, Indo-European, we likewise observe a steady increase of the reconstructed probability of labiodental sounds, from a median estimate of about 3% in the proto-language (6000 to 8000 years ago) to a presence of 76% in extant languages.<br />
CONCLUSION<br />
Our findings reveal that the transition from prehistoric foragers to contemporary societies has had an impact on the human speech apparatus, and therefore on our species’ main mode of communication and social differentiation: spoken language.</p>
</li>
</ul>
</li>
</ul>
<h2>00702</h2>
<ul>
<li>put consonants at end of held vowels — the consonants cannot sustain singing notes. i find that the consonants at the end of one word and rhe beginning of the next tend to run together when i do this</li>
</ul>
<h2>00602</h2>
<ul>
<li>practice breathing until you can make normal breaths last thirty seconds. sing continuously for thirty seconds by using minimal breath — the comparison is to singing in front of a lit candle, the flame should not move from the movement of your breath (because it is so gentle)</li>
</ul>
<h2>00502</h2>
<ul>
<li>listen to music on headphones at very lowest volume, can (learn to?) discern all the details of the music even at that volume. be aware though, that in my experience this makes you much more sensitive to sound than most people, so you should invest in decent earmuffs and also learn strategies for dealing with excess sound because you cannot expect other people to understand what it is like and they will not change their (now) noisy ways to accomodate you</li>
</ul>
<h2>00402</h2>
<h2>00302</h2>
<h2>00202</h2>
<h2>00102</h2>
<ul>
<li>i didn’t become the way i am because of some innate special ability. it was circumstance that i happened to live, and relative isolation no different to what many in the world are feeling right now. they say that birds raised alone sing defective songs, but after a few generations of living together in a society of many birds, from those originally isolated birds, their song emerges once again.</li>
<li>humankind has lived through many disasters like this, that destroyed our song. we are living through such a disaster now, as awful curses sweep through us, destroying our minds. yet even so, some of us through circumstance are loving one another and fragments of song are emerging once more.</li>
<li>we are each a fragment of a song of love — let us join our fragments together</li>
</ul>
<h2>00002</h2>
<ul>
<li>helium He (ἥλιος helios, sun)</li>
<li>β
<ul>
<li>Β, beta</li>
</ul>
</li>
<li>priss asagiri
<ul>
<li>knuckle bomber punch</li>
<li>blue hard suit</li>
</ul>
</li>
<li>OB
<ul>
<li>&quot;--- -...&quot;</li>
</ul>
</li>
</ul>

</div>

</body>
</html>
